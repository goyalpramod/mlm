# Optimization

## Optimization Fundamentals

### Unconstrained vs Constrained Optimization

### Convex vs Non-convex Functions

### Local vs Global Minima

### Optimization Landscape and Convergence Guarantees

## Unconstrained Optimization

### Gradient Descent

#### Learning Rate and Convergence

### Line Search Methods

### Newton's Method and Quasi-Newton Methods

### Coordinate Descent

## Advanced Gradient Methods

### Gradient Descent with Momentum

### Stochastic Gradient Descent

### Modern Optimizers

Adam, RMSprop, AdaGrad

## Constrained Optimization

### Constraint Optimization Fundamentals

### Lagrange Multipliers

### Minimax Theorem

## Convex Optimization

### Properties of Convex Functions

### Convex Optimization Algorithms

### Duality Theory