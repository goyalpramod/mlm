---
title: "Probability Theory"
description: "Learn probability fundamentals, random variables, and distributions essential for machine learning."
order: 3
estimatedTime: "50 minutes"
difficulty: "Beginner"
prerequisites: ["Basic calculus", "Set theory"]
learningObjectives:
  - "Understand probability fundamentals"
  - "Work with random variables and distributions"
  - "Apply Bayes' theorem"
  - "Model uncertainty in machine learning"
keywords: ["probability theory", "random variables", "distributions", "bayes theorem", "machine learning"]
lastUpdated: "2025-01-07"
---

export const metadata = {
  title: frontmatter.title,
  description: frontmatter.description,
  keywords: frontmatter.keywords,
}

# Probability Theory

Probability theory provides the mathematical framework for reasoning under uncertainty. In machine learning, we use probability to model uncertainty in data, make predictions, and quantify confidence in our results.

## Probability Fundamentals

A **sample space** Ω is the set of all possible outcomes of an experiment. An **event** is a subset of the sample space.

For example, when flipping a coin twice:
- Sample space: `Ω = {HH, HT, TH, TT}`
- Event "at least one head": `A = {HH, HT, TH}`

### Probability Rules

Probability assigns a number between 0 and 1 to each event. Key axioms include:

```
0 ≤ P(A) ≤ 1 for any event A
P(Ω) = 1 (certainty)
P(A ∪ B) = P(A) + P(B) - P(A ∩ B)
```

**Conditional Probability:** The probability of event A given that event B has occurred:

```
P(A|B) = P(A ∩ B) / P(B), provided P(B) > 0
```

## Probability Distributions

Probability distributions describe how probability is distributed over the possible values of a random variable.

### Discrete Distributions

**Bernoulli Distribution:** Models a single trial with success probability p:

```
P(X = k) = p^k × (1-p)^(1-k), k ∈ {0,1}
```

**Binomial Distribution:** Models the number of successes in n independent Bernoulli trials:

```
P(X = k) = C(n,k) × p^k × (1-p)^(n-k)
```

### Normal Distribution

The normal (Gaussian) distribution is the most important distribution in statistics and machine learning:

```
f(x) = (1/(σ√(2π))) × e^(-(x-μ)²/(2σ²))
```

## Bayes' Theorem

Bayes' theorem is fundamental to machine learning, providing a way to update our beliefs about hypotheses as we observe new evidence:

```
P(H|E) = P(E|H) × P(H) / P(E)
```

Where:
- `P(H|E)` is the posterior probability (updated belief)
- `P(E|H)` is the likelihood (how well hypothesis explains evidence)
- `P(H)` is the prior probability (initial belief)
- `P(E)` is the evidence (normalizing constant)

> **Note:** This is placeholder content for demonstration. Full mathematical content with interactive visualizations will be added separately.